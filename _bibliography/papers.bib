---
---
@inproceedings{HaiderISCAS2026,
  author       = {Haider, Muhammad Hamis and Zhang, H. and Ko, Seok-Bum},
  title        = {Power-Efficient and Reconfigurable Compute Unit for Multi-Precision AI Inference at the Edge},
  booktitle    = {Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)},
  year         = {2026},
  note         = {Accepted, awaiting publication},
  organization = {IEEE},
  preview      = {ftc_3off.png}
}



@inproceedings{HaiderAPCCAS2025,
  author       = {Haider, Muhammad Hamis and Kim, Nam Joon and Zhang, H. and Arias-Garcia, J. and Lee, Hyuk-Jae and Ko, Seok-Bum},
  title        = {Memory-Efficient Differential Privacy Accelerator},
  booktitle    = {Proceedings of the IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)},
  year         = {2025},
  organization = {IEEE},
  preview      = {apccas_architecture_fig.png}
}

@article{haider2023approx,
  html     = {https://ieeexplore.ieee.org/abstract/document/10005129},
  author   = {Haider, Muhammad Hamis and Ko, Seok-Bum},
  journal  = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  title    = {Booth Encoding-Based Energy Efficient Multipliers for Deep Learning Systems},
  year     = {2023},
  volume   = {70},
  number   = {6},
  pages    = {2241-2245},
  keywords = {Quantization (signal);Encoding;Hardware;Complexity theory;Delays;Convolutional neural networks;Pins;Edge computing;deep neural networks;approximate computing;booth multipliers;quantization;shift multipliers},
  doi      = {10.1109/TCSII.2022.3233923},
  preview  = {po2_mult.png}
}


@article{haider2023decoder,
  author   = {Haider, Muhammad Hamis and Zhang, Hao and Ko, Seok-Bum},
  journal  = {IEEE Transactions on Computers},
  title    = {Decoder Reduction Approximation Scheme for Booth Multipliers},
  year     = {2024},
  volume   = {73},
  number   = {3},
  pages    = {735-746},
  keywords = {Decoding;Error analysis;Computer architecture;Hardware;Adders;Filtration;Complexity theory;Booth multipliers;approximate computing;convolutional neural networks;logarithmic multipliers;leading one detection},
  doi      = {10.1109/TC.2023.3343093},
  preview  = {param_btrunc_fig.png}
}

@incollection{haider2024neuromorphic,
  title     = {Is Neuromorphic Computing the Key to Power-Efficient Neural Networks: A Survey},
  author    = {Haider, Muhammad Hamis and Zhang, Hao and Deivalaskhmi, S and Lakshmi Narayanan, G and Ko, Seok-Bum},
  booktitle = {Design and Applications of Emerging Computer Systems},
  pages     = {91--113},
  year      = {2024},
  publisher = {Springer}
}

@inproceedings{haider2024optimized,
  author    = {Haider, Muhammad Hamis and Valarezo-Plaza, Stephany and Muhsin, Sayed and Zhang, Hao and Ko, Seok-Bum},
  booktitle = {2024 IEEE International Symposium on Circuits and Systems (ISCAS)},
  title     = {Optimized Transformer Models: ℓ′ BERT with CNN-like Pruning and Quantization},
  year      = {2024},
  volume    = {},
  number    = {},
  pages     = {1-5},
  keywords  = {Quantization (signal);Sensitivity;Data security;Pipelines;Neural networks;Transformers;Convolutional neural networks;Transformers;BERT;Natural Language Processing;Optimization Techniques;Model Compression;Quantization;Pruning;Compression Methods for Deep Learning},
  doi       = {10.1109/ISCAS58744.2024.10558045},
  preview   = {err_sens_plot.png}
}

@article{valarezo2026fft,
  title    = {FFT-based deep learning for efficient combustion instability prediction: A comparative study of time and frequency-domain approaches},
  journal  = {International Journal of Hydrogen Energy},
  volume   = {197},
  pages    = {152615},
  year     = {2026},
  issn     = {0360-3199},
  doi      = {https://doi.org/10.1016/j.ijhydene.2025.152615},
  url      = {https://www.sciencedirect.com/science/article/pii/S0360319925056186},
  author   = {Stephany Valarezo-Plaza and Andres Erazo and Muhammad Hamis Haider and Jinhyun Bae and Pervez Canteenwalla and Sean Yun and SeokBum Ko},
  keywords = {Combustion instability, Hydrogen-enriched flame, Artificial intelligence, LSTM, FFT},
  abstract = {This study presents a deep learning-based approach for real-time prediction of combustion instability. Two LSTM models were developed: one trained on time-series data of pressure and heat release rate (OH* intensity), and another on frequency-domain features derived via fast Fourier transform (FFT). The dataset includes measurements taken under varying power levels (15–30 kW), hydrogen content (0%–80%), air flow rates (400–600 slpm), and downstream acoustic conditions with blockage ratios of 0, 0.73, and 0.85. Both models demonstrated high accuracy within a 100 ms window, 93.67% for the time-series model and 95.11% for the FFT-based model. However, the FFT-based model achieved 4.8× faster inference, making it more suitable for real-time deployment. Even at smaller windows, it maintained comparable accuracy (94.28% vs. 94.76%). Additionally, both models were tested on transitional regimes (stable to unstable and vice versa) labeled using the Rayleigh Index. The models showed strong alignment with these transitions, particularly for unstable-to-stable cases, confirming their reliability in dynamic operating conditions.}
}

@article{haider2025reconfigurable,
  title    = {Reconfigurable Multi-Precision Multipliers for Energy-Efficient CNN Acceleration for Visual AI in ICT Systems},
  journal  = {INTERNATIONAL JOURNAL OF CONTENTS},
  volume   = {21},
  number   = {4},
  page     = {1-9},
  year     = {2025},
  issn     = {2093-7504},
  doi      = {},
  url      = {https://ijcon/v.21/4/1/58185},
  author   = {Muhammad Hamis Haider and Lee, Hyuk Jae and Seok-Bum Ko},
  keywords = {CNN Acceleration,Multi-precision Pruning,Reconfigurable Multipliers,Energy Efficiency,Hardware-aware Inference},
  abstract = {Modern convolutional neural networks (CNNs) are essential in information and communication technology (ICT) applications, including edge computing, IoT devices, and mobile platforms, where energy efficiency and throughput are critical. These systems increasingly utilize multi-precision arithmetic to optimize accuracy and resource efficiency. However, traditional methods that assign separate fixed-precision multipliers for different bit-widths are inefficient, as the largest multiplier often dominates the critical path, limiting overall performance. In this paper, we introduce two scalable, power-efficient multiplier architectures with runtime reconfigurability: R4RC16 and R4RC32. These architectures are designed for CNN acceleration under multi-precision pruning. Each design features a low-power mode (8-bit) and a default mode (16-bit for R4RC16 and 32-bit for R4RC32), allowing for dynamic precision adjustment during inference with minimal overhead. When operating in low-power mode, our proposed multipliers achieve up to 7.6× greater energy efficiency compared to state-of-the-art approximate logarithmic multipliers, and up to 13.8× compared to approximate Booth-based designs. Additionally, they provide 2× (for 16-bit) and 4× (for 32-bit) higher throughput than exact 8-bit multipliers when processing pruned CNN workloads. Notably, the overhead in low-power mode is nearly independent of the full bit-width, resulting in a nearly constant power-delay product across both 16-bit and 32-bit designs. These findings highlight the significance of reconfigurable arithmetic units as critical components of ICT infrastructure that support healthcare, education, and multimedia, enabling CNNs to dynamically balance accuracy, energy, and throughput with less than 1% area overhead.},
  preview  = {ijoc.png}
}

